{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce50fd4f",
   "metadata": {},
   "source": [
    "# CNN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54af6e7",
   "metadata": {},
   "source": [
    "# 1. Import Libraries.\n",
    "\n",
    "In this section we Import some of the initial Libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ba318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d02c0c7",
   "metadata": {},
   "source": [
    "# 2. Lets use our Classifier dataset for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ad5b1",
   "metadata": {},
   "source": [
    "\"X_Ylabels_17_21.csv\"\n",
    "\n",
    "Binary labels for the years 2017 to 2021 signifying whether each stock outperformed (1) or underperformed (0) the equal weighted Nasdaq 100 that year.\n",
    "\n",
    "See: https://github.com/SheepBoss/Project-on-ML-dataset-and-models-for-stock-performance-predictions-based-on-financial-ratios/blob/main/ML_dataset_for%20Nasdaq100stocks_financial%20ratios_with%20labels.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da317fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset_turnover</th>\n",
       "      <th>buyback_yield</th>\n",
       "      <th>capex_to_revenue</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>cash_to_debt</th>\n",
       "      <th>cogs_to_revenue</th>\n",
       "      <th>mscore</th>\n",
       "      <th>zscore</th>\n",
       "      <th>current_ratio</th>\n",
       "      <th>days_inventory</th>\n",
       "      <th>...</th>\n",
       "      <th>price_to_earnings_ratio_nri</th>\n",
       "      <th>price_earnings_growth_ratio</th>\n",
       "      <th>price_to_free_cashflow</th>\n",
       "      <th>price_to_operating_cashflow</th>\n",
       "      <th>rate_of_return</th>\n",
       "      <th>scaled_net_operating_assets</th>\n",
       "      <th>yoy_ebitda_growth</th>\n",
       "      <th>yoy_eps_growth</th>\n",
       "      <th>yoy_revenue_growth</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>4.090000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>3.620000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.730000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>15.630000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>19.240000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.360000</td>\n",
       "      <td>10.830000</td>\n",
       "      <td>11.340000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.724932</td>\n",
       "      <td>2.035574</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>1.410274</td>\n",
       "      <td>2.648413</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-2.543382</td>\n",
       "      <td>6.686479</td>\n",
       "      <td>2.332192</td>\n",
       "      <td>89.855094</td>\n",
       "      <td>...</td>\n",
       "      <td>44.909524</td>\n",
       "      <td>10.936250</td>\n",
       "      <td>33.836721</td>\n",
       "      <td>35.271493</td>\n",
       "      <td>14.260952</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>26.097361</td>\n",
       "      <td>46.020147</td>\n",
       "      <td>19.277917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.540000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1.650000</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>-2.490000</td>\n",
       "      <td>10.810000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>89.855094</td>\n",
       "      <td>...</td>\n",
       "      <td>52.320000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>32.410000</td>\n",
       "      <td>30.420000</td>\n",
       "      <td>21.820000</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>39.030000</td>\n",
       "      <td>45.690000</td>\n",
       "      <td>25.510000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.490000</td>\n",
       "      <td>2.640000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>81.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>39.870000</td>\n",
       "      <td>4.440000</td>\n",
       "      <td>33.680000</td>\n",
       "      <td>27.720000</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>25.600000</td>\n",
       "      <td>-17.030000</td>\n",
       "      <td>36.640000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.724932</td>\n",
       "      <td>2.035574</td>\n",
       "      <td>0.086892</td>\n",
       "      <td>1.410274</td>\n",
       "      <td>2.648413</td>\n",
       "      <td>0.437534</td>\n",
       "      <td>-2.543382</td>\n",
       "      <td>6.686479</td>\n",
       "      <td>2.332192</td>\n",
       "      <td>89.855094</td>\n",
       "      <td>...</td>\n",
       "      <td>44.909524</td>\n",
       "      <td>10.936250</td>\n",
       "      <td>33.836721</td>\n",
       "      <td>35.271493</td>\n",
       "      <td>14.260952</td>\n",
       "      <td>0.482917</td>\n",
       "      <td>26.097361</td>\n",
       "      <td>46.020147</td>\n",
       "      <td>19.277917</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>1.570000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>-2.260000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>28.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.070000</td>\n",
       "      <td>6.195821</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>7.910000</td>\n",
       "      <td>-5.110000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>463.460000</td>\n",
       "      <td>10.370000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>1.733289</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-3.240000</td>\n",
       "      <td>6.120000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>83.647794</td>\n",
       "      <td>...</td>\n",
       "      <td>63.153488</td>\n",
       "      <td>6.195821</td>\n",
       "      <td>53.290000</td>\n",
       "      <td>42.510000</td>\n",
       "      <td>12.240260</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>156.700000</td>\n",
       "      <td>43.870000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>0.240000</td>\n",
       "      <td>-0.990000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>-1.792316</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>26.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.870000</td>\n",
       "      <td>8.730000</td>\n",
       "      <td>52.026374</td>\n",
       "      <td>16.700000</td>\n",
       "      <td>-1.040000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>6.090000</td>\n",
       "      <td>13.930000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>0.810000</td>\n",
       "      <td>-1.810000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>3.370000</td>\n",
       "      <td>40.040000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>-0.480000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>83.647794</td>\n",
       "      <td>...</td>\n",
       "      <td>165.360000</td>\n",
       "      <td>6.195821</td>\n",
       "      <td>80.070000</td>\n",
       "      <td>75.390000</td>\n",
       "      <td>12.240260</td>\n",
       "      <td>-0.220000</td>\n",
       "      <td>17.709700</td>\n",
       "      <td>52.306596</td>\n",
       "      <td>263.130000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>0.330000</td>\n",
       "      <td>1.733289</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>-2.670000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>83.647794</td>\n",
       "      <td>...</td>\n",
       "      <td>63.153488</td>\n",
       "      <td>6.195821</td>\n",
       "      <td>222.560000</td>\n",
       "      <td>158.440000</td>\n",
       "      <td>12.240260</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-86.740000</td>\n",
       "      <td>-116.850000</td>\n",
       "      <td>48.790000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     asset_turnover  buyback_yield  capex_to_revenue  cash_ratio  \\\n",
       "0          0.660000       4.090000          0.050000    0.740000   \n",
       "1          0.724932       2.035574          0.040000    1.410274   \n",
       "2          0.540000       1.080000          0.020000    1.650000   \n",
       "3          0.360000       0.140000          0.040000    0.660000   \n",
       "4          0.724932       2.035574          0.086892    1.410274   \n",
       "..              ...            ...               ...         ...   \n",
       "505        1.570000       0.250000          0.010000    0.050000   \n",
       "506        0.560000       1.733289          0.060000    0.830000   \n",
       "507        0.240000      -0.990000          0.320000    0.030000   \n",
       "508        0.810000      -1.810000          0.030000    3.370000   \n",
       "509        0.330000       1.733289          0.090000    2.090000   \n",
       "\n",
       "     cash_to_debt  cogs_to_revenue    mscore     zscore  current_ratio  \\\n",
       "0        0.640000         0.620000 -2.500000   3.620000       1.280000   \n",
       "1        2.648413         0.250000 -2.543382   6.686479       2.332192   \n",
       "2        3.090000         0.140000 -2.490000  10.810000       2.050000   \n",
       "3        0.130000         0.400000 -1.490000   2.640000       1.470000   \n",
       "4        2.648413         0.437534 -2.543382   6.686479       2.332192   \n",
       "..            ...              ...       ...        ...            ...   \n",
       "505      0.040000         0.790000 -2.260000   2.720000       0.720000   \n",
       "506      1.580000         0.280000 -3.240000   6.120000       1.120000   \n",
       "507      0.010000         0.610000 -1.792316   1.030000       0.840000   \n",
       "508     40.040000         0.310000 -0.480000  47.500000       3.800000   \n",
       "509      1.560000         0.220000 -2.670000  11.580000       2.570000   \n",
       "\n",
       "     days_inventory  ...  price_to_earnings_ratio_nri  \\\n",
       "0          9.040000  ...                    16.730000   \n",
       "1         89.855094  ...                    44.909524   \n",
       "2         89.855094  ...                    52.320000   \n",
       "3         81.440000  ...                    39.870000   \n",
       "4         89.855094  ...                    44.909524   \n",
       "..              ...  ...                          ...   \n",
       "505       28.090000  ...                    22.070000   \n",
       "506       83.647794  ...                    63.153488   \n",
       "507       26.040000  ...                    22.870000   \n",
       "508       83.647794  ...                   165.360000   \n",
       "509       83.647794  ...                    63.153488   \n",
       "\n",
       "     price_earnings_growth_ratio  price_to_free_cashflow  \\\n",
       "0                       1.250000               15.630000   \n",
       "1                      10.936250               33.836721   \n",
       "2                       2.420000               32.410000   \n",
       "3                       4.440000               33.680000   \n",
       "4                      10.936250               33.836721   \n",
       "..                           ...                     ...   \n",
       "505                     6.195821               10.530000   \n",
       "506                     6.195821               53.290000   \n",
       "507                     8.730000               52.026374   \n",
       "508                     6.195821               80.070000   \n",
       "509                     6.195821              222.560000   \n",
       "\n",
       "     price_to_operating_cashflow  rate_of_return  scaled_net_operating_assets  \\\n",
       "0                      12.600000       19.240000                     0.550000   \n",
       "1                      35.271493       14.260952                     0.482917   \n",
       "2                      30.420000       21.820000                     0.360000   \n",
       "3                      27.720000       11.950000                     2.130000   \n",
       "4                      35.271493       14.260952                     0.482917   \n",
       "..                           ...             ...                          ...   \n",
       "505                     7.910000       -5.110000                     0.640000   \n",
       "506                    42.510000       12.240260                     0.290000   \n",
       "507                    16.700000       -1.040000                     0.750000   \n",
       "508                    75.390000       12.240260                    -0.220000   \n",
       "509                   158.440000       12.240260                    -0.010000   \n",
       "\n",
       "     yoy_ebitda_growth  yoy_eps_growth  yoy_revenue_growth  0  \n",
       "0             9.360000       10.830000           11.340000  0  \n",
       "1            26.097361       46.020147           19.277917  0  \n",
       "2            39.030000       45.690000           25.510000  1  \n",
       "3            25.600000      -17.030000           36.640000  0  \n",
       "4            26.097361       46.020147           19.277917  1  \n",
       "..                 ...             ...                 ... ..  \n",
       "505          65.800000      463.460000           10.370000  0  \n",
       "506         156.700000       43.870000           14.100000  0  \n",
       "507           3.480000        6.090000           13.930000  1  \n",
       "508          17.709700       52.306596          263.130000  0  \n",
       "509         -86.740000     -116.850000           48.790000  0  \n",
       "\n",
       "[510 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Y = pd.read_csv(\"X_Ylabels_17_21.csv\")\n",
    "X_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32144a9",
   "metadata": {},
   "source": [
    "# Model: Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd74108",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fda3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\mark\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.11.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.2)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.20.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.19.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (2.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (15.0.6.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.51.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\mark\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54fd5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers, Sequential, optimizers, metrics\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import json\n",
    "import glob\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.metrics\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63e914",
   "metadata": {},
   "source": [
    "# Methodology: Shuffle and Split the data into X and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ae9255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shuffled = X_Y.iloc[np.random.permutation(len(X_Y))]\n",
    "\n",
    "num_rows = len(df_shuffled)\n",
    "df_train = df_shuffled[:int(num_rows * 0.85)]\n",
    "df_test = df_shuffled[int(num_rows * 0.85):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b3635d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcnn_train = df_train.iloc[:,:-1]\n",
    "Ycnn_train = df_train.iloc[:,-1]\n",
    "Xcnn_test = df_test.iloc[:,:-1]\n",
    "Ycnn_test = df_test.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5662d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the  data\n",
    "scaler = StandardScaler().fit(Xcnn_train)\n",
    "Xcnn_train = scaler.transform(Xcnn_train)\n",
    "Xcnn_test = scaler.transform(Xcnn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a84e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "        from tensorflow.python.ops.numpy_ops import np_config\n",
    "        np_config.enable_numpy_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226c7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcnn_train_reshaped = Xcnn_train.reshape(Xcnn_train.shape[0], -1)\n",
    "Xcnn_test_reshaped = Xcnn_test.reshape(Xcnn_test.shape[0], -1)\n",
    "\n",
    "# Apply normalization to the training and test data\n",
    "Xcnn_train_normalized = scaler.transform(Xcnn_train_reshaped)\n",
    "Xcnn_test_normalized = scaler.transform(Xcnn_test_reshaped)\n",
    "\n",
    "# Reshape the normalized data back to the original shape\n",
    "Xcnn_train_normalized = Xcnn_train_normalized.reshape(Xcnn_train.shape)\n",
    "Xcnn_test_normalized = Xcnn_test_normalized.reshape(Xcnn_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0d0c0",
   "metadata": {},
   "source": [
    "# Methodology: Convert data to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e388d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcnn_train = tf.convert_to_tensor(Xcnn_train_normalized)\n",
    "Ycnn_train = tf.convert_to_tensor(Ycnn_train)\n",
    "Xcnn_test = tf.convert_to_tensor(Xcnn_test_normalized)\n",
    "Ycnn_test = tf.convert_to_tensor(Ycnn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40449844",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcnn_train = tf.reshape(Xcnn_train,[433,1,39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2653d977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([433, 1, 39])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcnn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bc4a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([433])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ycnn_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a11d1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([77, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcnn_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd967ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcnn_test = tf.reshape(Xcnn_test,[77,1,39])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "949d76a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([77])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ycnn_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a82f3",
   "metadata": {},
   "source": [
    "# Model 2: Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b9108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1, 32)             6272      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1, 32)             0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1, 32)             5152      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 1, 32)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1, 32)             5152      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1, 64)             14400     \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 1, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 1, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 1, 64)             0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88,513\n",
      "Trainable params: 88,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=5, input_shape=(1, 39), padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=32, kernel_size=5, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Conv1D(filters=32, kernel_size=5, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Flatten(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06697caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model2.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Note: We tried a 'hinge' loss optimizer too.\n",
    "#model2.compile(optimizer=optimizer, loss='hinge', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb44b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5566\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6847 - accuracy: 0.5704\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6798 - accuracy: 0.5704\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5704\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6768 - accuracy: 0.5727\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6791 - accuracy: 0.5751\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6770 - accuracy: 0.5774\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.5774\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.5751\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6675 - accuracy: 0.5774\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6694 - accuracy: 0.5843\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.5797\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6618 - accuracy: 0.5843\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6647 - accuracy: 0.6074\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6576 - accuracy: 0.5889\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.5820\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.6166\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6583 - accuracy: 0.5889\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6630 - accuracy: 0.5751\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.6120\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6500 - accuracy: 0.6212\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6236\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.6467\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6285 - accuracy: 0.6490\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6290 - accuracy: 0.6536\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6615 - accuracy: 0.6074\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6365 - accuracy: 0.6189\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6374\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6295 - accuracy: 0.6374\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6407 - accuracy: 0.6212\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6190 - accuracy: 0.6397\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.6651\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6249 - accuracy: 0.6513\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6090 - accuracy: 0.6721\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.6721\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6242 - accuracy: 0.6513\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6143 - accuracy: 0.6628\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6031 - accuracy: 0.6513\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5992 - accuracy: 0.6397\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.6374\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6020 - accuracy: 0.6536\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5917 - accuracy: 0.6767\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5843 - accuracy: 0.6859\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.6559\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5958 - accuracy: 0.6790\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5788 - accuracy: 0.6836\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6039 - accuracy: 0.6490\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5747 - accuracy: 0.6882\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5659 - accuracy: 0.6928\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5774 - accuracy: 0.6790\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.6166\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.6928\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5603 - accuracy: 0.6952\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5717 - accuracy: 0.6952\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5542 - accuracy: 0.7021\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.7113\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5504 - accuracy: 0.7136\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7113\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5501 - accuracy: 0.6952\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.7044\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6605\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7067\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5583 - accuracy: 0.7090\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7021\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6280 - accuracy: 0.6490\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7067\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5424 - accuracy: 0.7275\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5482 - accuracy: 0.7206\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5474 - accuracy: 0.6905\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5084 - accuracy: 0.7367\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5067 - accuracy: 0.7275\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5362 - accuracy: 0.6975\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5275 - accuracy: 0.7275\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7252\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7413\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6270 - accuracy: 0.6397\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.6767\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.7344\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7460\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7460\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7644\n",
      "Epoch 82/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7367\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.7529\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7552\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7021\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7529\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4594 - accuracy: 0.7667\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7691\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4925 - accuracy: 0.7390\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7621\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7460\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7552\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7760\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.7714\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.7829\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.7829\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7598\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8060\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.7829\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4358 - accuracy: 0.7829\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(Xcnn_train, Ycnn_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bb7173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.5194805194805194\n"
     ]
    }
   ],
   "source": [
    "            ypreds = model2.predict(Xcnn_test)\n",
    "            ypreds_binary = [1 if y >= 0.5 else 0 for y in ypreds]\n",
    "            accuracy = accuracy_score(Ycnn_test, ypreds_binary)\n",
    "            print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e50ae324",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3fe70a0",
   "metadata": {},
   "source": [
    "# Model: Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4073cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025CD6D6CEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025CD7F2D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 769us/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1ms/step\n",
      "3/3 [==============================] - 0s 1000us/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "Best Accuracy: 0.5974025974025974\n",
      "Best Parameters: {'filters': 64, 'kernel_size': 3, 'dropout_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the parameter ranges for the CNN model\n",
    "param_range1 = [32, 64, 128]  # filters\n",
    "param_range2 = [3, 5, 7]  # kernel_size\n",
    "param_range3 = [0.2, 0.3, 0.4]  # dropout rate\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "for filters in param_range1:\n",
    "    for kernel_size in param_range2:\n",
    "        for dropout_rate in param_range3:\n",
    "            model = Sequential([\n",
    "                Conv1D(filters=filters, kernel_size=kernel_size, input_shape=(1, 39), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                Conv1D(filters=filters, kernel_size=kernel_size, padding='same'),\n",
    "                LeakyReLU(),\n",
    "                MaxPooling1D(pool_size=1),\n",
    "                Flatten(),\n",
    "                Dropout(dropout_rate),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            model.fit(Xcnn_train, Ycnn_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "            ypreds = model.predict(Xcnn_test)\n",
    "            ypreds_binary = [1 if y >= 0.5 else 0 for y in ypreds]\n",
    "\n",
    "            accuracy = accuracy_score(Ycnn_test, ypreds_binary)\n",
    "\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = {'filters': filters, 'kernel_size': kernel_size, 'dropout_rate': dropout_rate}\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0900f",
   "metadata": {},
   "source": [
    "   # Methodology: Adapt Model to Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bb229ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "     model3 = Sequential([\n",
    "                Conv1D(filters=64, kernel_size=3, input_shape=(1, 39), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                Conv1D(filters=64, kernel_size=3, padding='same'),\n",
    "                LeakyReLU(),\n",
    "                MaxPooling1D(pool_size=1),\n",
    "                Flatten(),\n",
    "                Dropout(0.4),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "473eef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbfdbdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25cd7f28580>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(Xcnn_train, Ycnn_train, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0064e8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "            ypreds = model3.predict(Xcnn_test)\n",
    "            ypreds_binary = [1 if y >= 0.5 else 0 for y in ypreds]\n",
    "            accuracy = accuracy_score(Ycnn_test, ypreds_binary)\n",
    "            print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6471c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6e84035",
   "metadata": {},
   "source": [
    "# Methodology: We run some optimisation on the Learning Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "825ddc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.5455\n",
      "Learning Rate: 0.001, Accuracy: 0.5454545617103577\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000025CD6E26160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.5455\n",
      "Learning Rate: 0.01, Accuracy: 0.5454545617103577\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x0000025CD912AA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.5584\n",
      "Learning Rate: 0.1, Accuracy: 0.5584415793418884\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    "\n",
    "# Define the learning rate scheduler function\n",
    "def lr_scheduler(epoch, lr):\n",
    "    # Define the schedule based on the epoch\n",
    "    if epoch < 10:\n",
    "        return lr  # Keep the initial learning rate for the first 10 epochs\n",
    "    else:\n",
    "        return lr * np.exp(-0.1)  # Reduce the learning rate exponentially after 10 epochs\n",
    "\n",
    "# Set the range of parameters to loop through\n",
    "param_range = [0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "# Loop through the parameters\n",
    "for learning_rate in param_range:\n",
    "    # Create the model\n",
    "    model = Sequential([\n",
    "                Conv1D(filters=32, kernel_size=5, input_shape=(1, 39), padding='same'),\n",
    "                LeakyReLU(),\n",
    "                Conv1D(filters=32, kernel_size=5, padding='same'),\n",
    "                LeakyReLU(),\n",
    "                MaxPooling1D(pool_size=1),\n",
    "                Flatten(),\n",
    "                Dropout(0.4),\n",
    "                Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "\n",
    "    # Compile the model with the desired learning rate\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Define the learning rate scheduler callback\n",
    "    lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(Xcnn_train, Ycnn_train, epochs=20, batch_size=32,verbose=0, callbacks=[lr_scheduler_callback])\n",
    "\n",
    "    # Evaluate the model on the test data\n",
    "    accuracy = model.evaluate(Xcnn_test, Ycnn_test)[1]\n",
    "    \n",
    "    print(f\"Learning Rate: {learning_rate}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc83d52",
   "metadata": {},
   "source": [
    "# Methodology: Trail Model with Higher Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b916ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model3.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "effdd69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25cd8019c40>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(Xcnn_train, Ycnn_train, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cce664a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step\n",
      "Accuracy: 0.5844155844155844\n"
     ]
    }
   ],
   "source": [
    "            ypreds = model3.predict(Xcnn_test)\n",
    "            ypreds_binary = [1 if y >= 0.5 else 0 for y in ypreds]\n",
    "            accuracy = accuracy_score(Ycnn_test, ypreds_binary)\n",
    "            print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0dcf20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99687e5b",
   "metadata": {},
   "source": [
    "# Model 1: Intuitive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e629951d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_80 (Conv1D)          (None, 1, 64)             17536     \n",
      "                                                                 \n",
      " leaky_re_lu_80 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_81 (Conv1D)          (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_81 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_38 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_82 (Conv1D)          (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_82 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_83 (Conv1D)          (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_83 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " max_pooling1d_39 (MaxPoolin  (None, 1, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_84 (Conv1D)          (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_84 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_85 (Conv1D)          (None, 1, 64)             28736     \n",
      "                                                                 \n",
      " leaky_re_lu_85 (LeakyReLU)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " flatten_36 (Flatten)        (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161,281\n",
      "Trainable params: 161,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=7, input_shape=(1, 39), padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    MaxPooling1D(pool_size=1),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Conv1D(filters=64, kernel_size=7, padding='same'),\n",
    "    LeakyReLU(),\n",
    "    Flatten(),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c415fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34b8190c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25cda232f10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(Xcnn_train, Ycnn_train, epochs=100, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1435a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step\n",
      "Accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "ypreds = model1.predict(Xcnn_test)\n",
    "ypreds_binary = [1 if y >= 0.5 else 0 for y in ypreds]\n",
    "accuracy = accuracy_score(Ycnn_test, ypreds_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375c3c5",
   "metadata": {},
   "source": [
    "# Investment Performance of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee50efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 1, 39)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the tensors vertically\n",
    "concatenated_tensor = tf.concat([Xcnn_train, Xcnn_test], axis=0)\n",
    "\n",
    "# Print the shape of the concatenated tensor\n",
    "print(concatenated_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31ca5b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([510])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yact = pd.read_csv(\"perfs17_21.csv\")\n",
    "Yact = tf.convert_to_tensor(Yact)\n",
    "Yact = tf.reshape(Yact,[510])\n",
    "Yact.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a844619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(510,), dtype=float64, numpy=\n",
       "array([ 1.32137313e-01,  0.00000000e+00,  2.33293169e-01,  1.55162952e-01,\n",
       "        4.07648932e-01,  2.40802716e-01,  2.21023538e-01,  1.32202369e-01,\n",
       "       -2.86818928e-01,  1.53930348e+00,  1.14382922e-01,  2.30360557e-01,\n",
       "        1.66060399e-01, -5.29310769e-02, -3.25081539e-01,  2.76087415e-01,\n",
       "        1.56133797e-01, -2.61391620e-01, -1.36732165e-01, -1.61258166e-01,\n",
       "        7.27223213e-01,  0.00000000e+00,  1.14677727e-01,  1.70032241e-01,\n",
       "        2.85039584e-01,  1.89672095e-01,  0.00000000e+00,  2.58801642e-01,\n",
       "        3.43026437e-01,  1.84839949e-01, -1.00000038e-01,  0.00000000e+00,\n",
       "        1.06849307e-01,  1.00000000e+00,  6.05986955e-01, -1.61745306e-01,\n",
       "       -7.70378270e-02,  2.85055117e-01,  1.78054541e-01,  0.00000000e+00,\n",
       "        5.67189366e-01, -1.37684008e-01,  1.37159696e-01,  1.34745613e-01,\n",
       "        1.47391176e-01,  1.68295140e-01,  3.14144340e-01,  3.11059692e-02,\n",
       "        5.07989584e-01,  3.82118509e-01, -2.55371737e-01, -7.63726981e-01,\n",
       "       -4.75838802e-01,  9.54041035e-02,  0.00000000e+00, -1.18871868e-01,\n",
       "        8.38756621e-01, -8.00853047e-02, -9.19439741e-02,  1.96261651e-01,\n",
       "        4.24647128e-01,  4.31817351e-02, -4.59709368e-02,  1.00000000e+00,\n",
       "       -5.28571719e-02,  2.92210001e-01,  2.73852438e-01, -2.07326384e-01,\n",
       "        2.07245608e-01, -1.38877961e-01, -2.24664276e-01, -2.44529920e-01,\n",
       "       -1.75546148e-02,  1.07603530e+00,  5.69649851e-01,  3.38034330e-01,\n",
       "        3.02159389e-01,  2.97717598e-02, -6.35722690e-01,  1.22766847e-01,\n",
       "        3.68656815e-01,  2.92365807e-02,  1.92414951e-01,  1.93895800e-01,\n",
       "        2.84159563e-01,  3.99312145e-01, -9.13461083e-02,  3.83349405e-01,\n",
       "        2.66388853e-01, -1.77338903e-01,  1.08438433e+00,  1.32044519e-01,\n",
       "        5.15912955e-02,  2.09837359e-02,  2.78846154e-01,  5.31376529e-01,\n",
       "        1.28666105e-01, -3.36031881e-02,  5.17189876e-01,  2.35927873e-01,\n",
       "        0.00000000e+00,  1.00000000e+00,  3.38720702e-01,  0.00000000e+00,\n",
       "        1.94191159e-01, -1.48380314e-01, -1.44359660e-01,  1.79693727e-03,\n",
       "       -4.50148853e-02, -3.88210864e-01,  1.55320219e-01,  7.82131612e-01,\n",
       "        6.71123291e-02,  9.48870053e-02,  2.72344112e-01,  3.91332148e-01,\n",
       "        3.06391415e-01, -2.11532663e-01,  1.04625266e-01, -3.88595710e-01,\n",
       "        3.38438101e-01, -2.29003261e-01,  3.98362882e-02,  0.00000000e+00,\n",
       "        2.57703707e-01, -1.40069998e-01,  1.77541941e-01,  1.30879625e-01,\n",
       "        1.00000000e+00, -2.71902201e-01, -2.34161990e-01, -1.42941959e-01,\n",
       "       -3.58592089e-01,  1.00000000e+00, -3.00552165e-01,  2.11163501e+00,\n",
       "        1.26087315e+00, -1.43658282e-02, -1.90630050e-01, -2.65709114e-01,\n",
       "       -2.81448861e-02,  0.00000000e+00,  2.04834971e-01,  1.49976922e-01,\n",
       "       -8.94906751e-03, -1.26945021e-02, -1.58129909e-01,  8.33631435e-02,\n",
       "       -1.20924417e-01,  7.82119484e-03, -1.20156089e-01, -1.32093600e-01,\n",
       "        3.43283599e-01, -1.32284553e-01, -2.42266505e-01,  2.03751789e-01,\n",
       "        0.00000000e+00,  3.40707264e-01,  1.56709640e-01, -4.01950598e-01,\n",
       "       -1.82738623e-01,  3.20520168e-03, -3.77169440e-02,  6.59911265e-04,\n",
       "        3.07804403e-02,  2.80943026e-01,  1.37757660e-01,  3.37205388e-01,\n",
       "        1.66578347e-01,  1.76627029e-02,  5.31186969e-02,  3.29302136e-01,\n",
       "        4.68033023e-01, -6.17716837e-02,  3.63598500e-01,  4.77819378e-01,\n",
       "       -2.24697406e-01, -3.24934136e-01, -2.15461340e-01, -1.02876407e-01,\n",
       "        4.52822576e-01, -1.99918769e-02, -7.80046106e-02,  1.86217832e-01,\n",
       "        1.89153023e-01, -6.58431510e-02, -1.15684677e-01,  5.75368659e-01,\n",
       "       -1.28747797e-01,  1.18454117e-01,  1.30819013e-02,  8.36565666e-02,\n",
       "        2.21282990e-01,  2.14182393e-01,  8.72364804e-01, -5.78862959e-02,\n",
       "        4.79699615e-02, -8.09650386e-03,  2.93558037e-01, -2.76908493e-01,\n",
       "       -3.24760191e-01,  7.27628575e-02,  1.00000000e+00,  1.85921959e-01,\n",
       "        9.21428365e-01,  1.00000000e+00,  4.93746888e-01,  7.29838251e-01,\n",
       "        3.78914335e-01,  7.75464337e-01,  5.90146711e-02,  2.11313618e+00,\n",
       "        1.91575746e+00,  7.26033439e-01,  2.27297404e-01,  5.86935505e-01,\n",
       "        4.60661573e-01,  1.35957786e+00,  5.63550785e-01,  9.55546148e-01,\n",
       "        1.13300524e-01,  1.15844827e+00, -1.15778508e-01,  7.31811153e-01,\n",
       "        1.07434893e+00,  0.00000000e+00,  4.14178045e-01,  5.73880132e-01,\n",
       "        2.36208063e-01,  5.85084733e-01,  3.24744335e+00,  3.15441294e-01,\n",
       "        6.82722448e-01,  9.70384454e-01,  6.81084586e-01,  1.60561143e+00,\n",
       "        5.57914757e-01,  1.19101725e+00,  3.34682769e-01,  3.51402592e-01,\n",
       "        1.03725891e+00,  1.88264024e-01,  6.08959961e-01,  0.00000000e+00,\n",
       "        8.22872405e-01, -1.35500329e-01,  7.78992352e-01,  7.75050557e-01,\n",
       "        6.22468183e-01,  1.01993885e+00,  4.06195095e-01,  1.82557303e-01,\n",
       "        6.65478250e-01,  4.92175006e-01,  1.08222227e+00,  4.16151557e-01,\n",
       "        6.16814890e-01,  1.29859455e+00,  1.00000000e+00,  1.48016663e+00,\n",
       "        6.18095418e-01,  9.79815605e-01,  1.28938045e+00,  1.68729965e-01,\n",
       "        2.01309930e+00,  7.65767346e-01,  6.19089923e-01,  4.34100204e+00,\n",
       "        1.16438362e+00,  4.94959066e-01,  1.08025444e+00,  1.09724194e+00,\n",
       "        3.89240940e-01,  6.08611665e-01,  1.02553103e+00,  1.42783065e+00,\n",
       "        8.31555796e-01,  8.02960812e-01,  6.84936125e-01,  9.64259617e-01,\n",
       "        5.57851232e-01,  5.20039235e-01,  2.71579266e+00,  1.77768516e-01,\n",
       "        1.53645291e+00,  9.59940774e-01, -3.10266310e-02,  3.78751316e-01,\n",
       "        6.62153943e-01,  2.03501507e-01,  2.32793539e-01,  9.23907226e-01,\n",
       "        7.32788729e-02,  1.05280825e+00,  5.35480132e-01,  4.93325371e-01,\n",
       "        5.37337746e+00,  8.91223906e-01,  2.67685437e-01,  1.03670379e-01,\n",
       "       -9.69111068e-02,  2.00000033e-01,  9.07771390e-01,  1.02985124e-01,\n",
       "        1.19880933e+00,  3.29483871e+00,  4.29471948e-01,  1.34128065e-01,\n",
       "       -4.15465852e-02,  6.51276162e-02,  2.07300853e-01, -2.26592059e-01,\n",
       "        1.77922081e-01, -1.94873827e-01, -1.34730761e-02,  3.92866195e-01,\n",
       "       -2.80936872e-02,  5.36088471e-02, -6.45246902e-02,  8.19133221e-02,\n",
       "       -1.38602144e-01,  3.58064072e-01,  3.34271816e-01, -3.91863934e-01,\n",
       "       -2.47184965e-01,  7.98761399e-03,  2.00525587e-01,  1.00000000e+00,\n",
       "       -1.15879546e-01, -1.34725574e-01,  6.33709594e-01,  1.55234329e-01,\n",
       "       -3.33773959e-02,  7.83213945e-02,  1.65214732e-01,  2.46345017e-01,\n",
       "        1.47849399e-01,  8.09325477e-01,  3.99178710e-01, -4.70881678e-01,\n",
       "        4.23523151e-01, -6.54501980e-02, -6.49902555e-02,  5.26686372e-01,\n",
       "        1.81384307e-01,  0.00000000e+00,  8.53052859e-01, -8.01484869e-02,\n",
       "        3.50164130e-01,  3.48520311e-01, -1.03607153e-01,  1.18023340e-01,\n",
       "       -9.02463258e-02, -2.25624979e-01,  2.55260271e-01,  2.24781385e-01,\n",
       "       -3.13767370e-01,  1.02705927e-01, -1.52500153e-02,  1.07929795e-01,\n",
       "        2.80119880e+00, -9.68181000e-02,  1.90799195e-01,  1.86618049e-01,\n",
       "       -3.18258111e-02,  7.26123371e-02, -1.92006216e-01, -2.45034456e-01,\n",
       "       -1.22845485e-01,  1.43112855e+00,  4.64066961e-01,  3.07672698e-01,\n",
       "       -2.08472893e-01, -1.16993536e-01, -2.81926896e-01, -1.31415838e-01,\n",
       "        1.04416301e+00, -8.07588895e-02,  2.42377556e-01, -3.15156686e-01,\n",
       "        3.50340112e-01,  9.32900652e-01,  3.92266944e-01, -5.21954410e-02,\n",
       "       -7.00403353e-01,  1.83315719e-01, -5.23760487e-01,  1.52575696e-01,\n",
       "        4.76138059e-01, -2.45600898e-01, -1.67475025e-01,  3.73757916e-02,\n",
       "        8.70278686e-02,  3.45023774e-01,  9.69147128e-02, -2.73599292e-01,\n",
       "        3.94144972e-01,  2.44233791e-02,  6.13342825e-01, -2.91550326e-02,\n",
       "        2.14726368e-01,  1.19239348e-01,  2.14435302e-01, -1.84517319e-01,\n",
       "       -3.61066949e-02,  8.50999226e-02, -6.35127143e-01,  6.08983025e-01,\n",
       "       -5.56096826e-02, -4.85554688e-01, -1.54185507e-01,  1.93970275e-01,\n",
       "       -2.15785732e-02, -2.88780138e-02, -8.80023963e-02, -2.33623820e-01,\n",
       "       -6.80576708e-02, -1.03621681e-01, -2.89501787e-04, -3.66309284e-01,\n",
       "        4.76939845e-02,  1.91337856e-02,  6.84058880e-02,  1.88349407e-02,\n",
       "        4.62768691e-02,  1.40740701e-01,  3.20180393e-01,  1.29430036e-01,\n",
       "        2.77453410e-01,  3.95555556e-01, -3.44460382e-01, -1.90303287e-01,\n",
       "       -1.37153745e-01,  1.98852289e-01, -4.85763126e-01, -6.24103237e-02,\n",
       "       -2.00534047e-01,  8.76606802e-02, -3.20508514e-01, -5.87333670e-01,\n",
       "       -1.03652772e-01, -4.55750581e-01, -9.16340976e-02, -4.79013919e-02,\n",
       "       -2.25113513e-01, -1.20512315e-01, -9.19192384e-02,  0.00000000e+00,\n",
       "       -2.76233467e-02,  3.95626580e-01, -2.55278423e-01, -2.54103225e-01,\n",
       "       -1.77819235e-02, -8.58772551e-02, -3.34430432e-01, -3.40799087e-01,\n",
       "       -7.28101032e-02, -1.53175526e-01, -2.41575943e-01, -6.91293573e-02,\n",
       "       -1.82787822e-02,  9.04496970e-02, -8.20499343e-01, -1.39320147e-02,\n",
       "       -2.84754405e-03, -5.52489713e-02,  1.14985353e-01,  1.10544801e-01,\n",
       "        1.08097724e-01, -4.68609379e-02,  3.51939857e-01, -2.92778959e-01,\n",
       "       -3.96179057e-01, -6.49022410e-02, -6.46956041e-01, -2.25317748e-01,\n",
       "       -7.77115930e-02, -1.39368934e-02,  1.79945904e-02,  7.56426347e-03,\n",
       "        1.41154428e-01, -4.28722880e-01,  2.39459170e-01, -3.74129950e-02,\n",
       "       -1.60328312e-01,  2.46735504e-01,  8.92296195e-01,  8.91384737e-02,\n",
       "       -3.43363585e-01, -1.65161656e-01,  1.76469750e-01,  1.73225717e-01,\n",
       "        1.44663030e-01,  4.05553606e-01, -4.00302100e-01,  1.58970243e-01,\n",
       "       -3.54821366e-01, -1.14795885e-01, -4.17452248e-01,  1.28476739e-01,\n",
       "       -4.22438754e-01,  1.37889625e-02, -1.06089567e-01, -5.00314868e-02,\n",
       "        2.07303544e-01, -2.27607740e-01, -1.37476040e-01, -6.55396404e-02,\n",
       "       -3.70127150e-01, -6.51759873e-01])>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019511d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "187060cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, X_data, perfs):\n",
    "    # Apply the model to the X data to generate predicted labels\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(Xcnn_train, Ycnn_train, epochs=100, batch_size=32, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict(X_data)\n",
    "    y_pred = [1 if y >= 0.5 else 0 for y in y_pred]  \n",
    "\n",
    "    # Convert perfs to a list with the desired shape\n",
    "    perfs = np.reshape(perfs, (-1, 1)).tolist()\n",
    "\n",
    "    # Create empty lists to store the actual performance values for each year\n",
    "    perf_1_list_Yr1 = []\n",
    "    perf_0_list_Yr1 = []\n",
    "    perf_1_list_Yr2 = []\n",
    "    perf_0_list_Yr2 = []\n",
    "    perf_1_list_Yr3 = []\n",
    "    perf_0_list_Yr3 = []\n",
    "    perf_1_list_Yr4 = []\n",
    "    perf_0_list_Yr4 = []\n",
    "    perf_1_list_Yr5 = []\n",
    "    perf_0_list_Yr5 = []\n",
    "\n",
    "    # Loop through the predicted labels and lookup the actual performances\n",
    "    for i, label in enumerate(y_pred):\n",
    "        # Get the actual performance for the corresponding label\n",
    "        actual_perf = perfs[i][0]  # Assuming perfs is a Python list with the actual performance values\n",
    "\n",
    "        # Determine the year based on the index i\n",
    "        year = i // 102 + 1\n",
    "\n",
    "        # Append the actual performance to the corresponding list based on the year and label\n",
    "        if year == 1:\n",
    "            if label == 1:\n",
    "                perf_1_list_Yr1.append(actual_perf)\n",
    "            else:\n",
    "                perf_0_list_Yr1.append(actual_perf)\n",
    "        elif year == 2:\n",
    "            if label == 1:\n",
    "                perf_1_list_Yr2.append(actual_perf)\n",
    "            else:\n",
    "                perf_0_list_Yr2.append(actual_perf)\n",
    "        elif year == 3:\n",
    "            if label == 1:\n",
    "                perf_1_list_Yr3.append(actual_perf)\n",
    "            else:\n",
    "                perf_0_list_Yr3.append(actual_perf)\n",
    "        elif year == 4:\n",
    "            if label == 1:\n",
    "                perf_1_list_Yr4.append(actual_perf)\n",
    "            else:\n",
    "                perf_0_list_Yr4.append(actual_perf)\n",
    "        elif year == 5:\n",
    "            if label == 1:\n",
    "                perf_1_list_Yr5.append(actual_perf)\n",
    "            else:\n",
    "                perf_0_list_Yr5.append(actual_perf)\n",
    "\n",
    "    # Calculate the mean performance for each list if they have non-zero lengths\n",
    "    mean_perf_1_Yr1 = sum(perf_1_list_Yr1) / len(perf_1_list_Yr1) if len(perf_1_list_Yr1) > 0 else 0\n",
    "    mean_perf_0_Yr1 = sum(perf_0_list_Yr1) / len(perf_0_list_Yr1) if len(perf_0_list_Yr1) > 0 else 0\n",
    "    mean_perf_1_Yr2 = sum(perf_1_list_Yr2) / len(perf_1_list_Yr2) if len(perf_1_list_Yr2) > 0 else 0\n",
    "    mean_perf_0_Yr2 = sum(perf_0_list_Yr2) / len(perf_0_list_Yr2) if len(perf_0_list_Yr2) > 0 else 0\n",
    "    mean_perf_1_Yr3 = sum(perf_1_list_Yr3) / len(perf_1_list_Yr3) if len(perf_1_list_Yr3) > 0 else 0\n",
    "    mean_perf_0_Yr3 = sum(perf_0_list_Yr3) / len(perf_0_list_Yr3) if len(perf_0_list_Yr3) > 0 else 0\n",
    "    mean_perf_1_Yr4 = sum(perf_1_list_Yr4) / len(perf_1_list_Yr4) if len(perf_1_list_Yr4) > 0 else 0\n",
    "    mean_perf_0_Yr4 = sum(perf_0_list_Yr4) / len(perf_0_list_Yr4) if len(perf_0_list_Yr4) > 0 else 0\n",
    "    mean_perf_1_Yr5 = sum(perf_1_list_Yr5) / len(perf_1_list_Yr5) if len(perf_1_list_Yr5) > 0 else 0\n",
    "    mean_perf_0_Yr5 = sum(perf_0_list_Yr5) / len(perf_0_list_Yr5) if len(perf_0_list_Yr5) > 0 else 0\n",
    "\n",
    "    # Calculate the 5-year performance for label 1 and label 0\n",
    "    five_year_perf_1 = (1 + mean_perf_1_Yr1) * (1 + mean_perf_1_Yr2) * (1 + mean_perf_1_Yr3) * (1 + mean_perf_1_Yr4) * (1 + mean_perf_1_Yr5)\n",
    "    five_year_perf_0 = (1 + mean_perf_0_Yr1) * (1 + mean_perf_0_Yr2) * (1 + mean_perf_0_Yr3) * (1 + mean_perf_0_Yr4) * (1 + mean_perf_0_Yr5)\n",
    "    five_year_perf_LS = (1 + (mean_perf_1_Yr1 - mean_perf_0_Yr1)) * (1 + (mean_perf_1_Yr2 - mean_perf_0_Yr2)) * (1 + (mean_perf_1_Yr3 - mean_perf_0_Yr3)) * (1 + (mean_perf_1_Yr4 - mean_perf_0_Yr4)) * (1 + (mean_perf_1_Yr5 - mean_perf_0_Yr5))\n",
    "    cagr_Long = (five_year_perf_1 / 1) ** (1 / 5) - 1\n",
    "    cagr_Short = (five_year_perf_0 / 1) ** (1 / 5) - 1\n",
    "    cagr_LongShort = (five_year_perf_LS / 1) ** (1 / 5) - 1\n",
    "\n",
    "    #Print the lists and the mean and 5-year performance values\n",
    "    print(\"5-year performance (Label 1):\", five_year_perf_1)\n",
    "    print(\"5-year performance Long on 1s cagr:\", cagr_Long)\n",
    "    print(\"5-year performance (Label 0):\", five_year_perf_0)\n",
    "    print(\"5-year performance Long on Os cagr:\", cagr_Short)\n",
    "    print(\"5-year performance LS:\", five_year_perf_LS)\n",
    "    print(\"5-year performance LS cagr:\", cagr_LongShort)\n",
    "    \n",
    "    #print(perf_1_list_Yr1)\n",
    "    #print(perf_0_list_Yr1)\n",
    "    #print(perf_1_list_Yr2)\n",
    "    #print(perf_0_list_Yr2)\n",
    "    #print(perf_1_list_Yr3)\n",
    "    #print(perf_0_list_Yr3)\n",
    "\n",
    "    return cagr_Long, cagr_Short, cagr_LongShort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15d66716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step\n",
      "5-year performance (Label 1): 2.2702879848081894\n",
      "5-year performance Long on 1s cagr: 0.17819232722689726\n",
      "5-year performance (Label 0): 2.602951469403655\n",
      "5-year performance Long on Os cagr: 0.21085799650770332\n",
      "5-year performance LS: 0.8690534156727167\n",
      "5-year performance LS cagr: -0.0276798317210597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.17819232722689726, 0.21085799650770332, -0.0276798317210597)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model3, concatenated_tensor, Yact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd10e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c87ab432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step\n",
      "5-year performance (Label 1): 2.3235905154853658\n",
      "5-year performance Long on 1s cagr: 0.18367348433280317\n",
      "5-year performance (Label 0): 2.534755317417298\n",
      "5-year performance Long on Os cagr: 0.2044456530518972\n",
      "5-year performance LS: 0.8124661954608012\n",
      "5-year performance LS cagr: -0.04068538695608692\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create a list of metric names\n",
    "metrics = ['Accuracy', 'Label 1 cagr %', 'Label 0 cagr %', 'Long Short cagr %']\n",
    "\n",
    "cagr_Long, cagr_Short, cagr_LongShort = evaluate_model(model3, concatenated_tensor, Yact)\n",
    "\n",
    "# Create a list of metric scores\n",
    "scores = [accuracy, cagr_Long , cagr_Short , cagr_LongShort ]\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.bar(metrics, scores)\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Display the score numbers on the bars\n",
    "for i, score in enumerate(scores):\n",
    "    plt.text(i, score + 0.01, str(round(score, 2)), ha='center')\n",
    "\n",
    "# Set the y-axis label\n",
    "plt.ylabel('Score')\n",
    "\n",
    "# Set the plot title\n",
    "plt.title('CNN Classifier Performance Scores')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66957d6c",
   "metadata": {},
   "source": [
    "# Appendix: Workings for future models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360b1cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
